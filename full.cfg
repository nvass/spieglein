# list of jobs that will be executed
active="backupserver www ftp localbackupserver"

# setting inparallel to an non-empty string will execute all jobs in parallel
inparallel=

# lock each job with the following command. FreeBSD example:
lockf=lockf
lockf_params="-t 10 /tmp/\$job /bin/sh \$backup_function_params"

# lock each job with the following command. Linux example:
#lockf=flock
#lockf_params="-w 10 /tmp/\$job /bin/sh \$backup_function_params"

# setting verbose to an non-empty string will print more messages
#verbose=

# filter snapshots. Without a filter the latest snapshot is always copied
#filter=egrep
#filter_params="_(06|12|18)-30-"

# Receiving datasets without the u parameter is potentially dangerous
# because incoming datasets might be mounted on the backup server and
# mask local filesystems.
# Normally you *never* want to change this default.
recv_nomount_param=-u
export recv_nomount_param

# set compression globally
compress=xz
decompress=unxz

# set ssh_user globally
ssh_user=backup

# export all globals that we need
#export filter filter_params
export ssh_user
export compress compress_params
export decompress decompress_params

# Job definitions. Variables in job definitions will be evaluated and mask global values.
# Setting a value to an empty string e.g. compress= can be used to effectively delete a global value
backupserver='
	filter=grep
	filter_params="_06-30-"
	ssh_host=backupserver.somedomain.tld
	ssh_params="-o port=3333"
	ssh_user=backup
	datasets="clio|backup/clio storage|backup/storage"
'

www='
	ssh_host=www.somedomain.tld
	ssh_user=backup
	datasets="pool/backups|backup/www"
'

ftp='
	ssh_host=ftp.somedomain.tld
	ssh_user=backup
	datasets="ftp|backup/ftp"
'

# override compression and override ssh_user
# copy pool/backups to backup/localbackupserver/backups
localbackupserver='
	compress=
	ssh_host=10.32.0.10
	ssh_user=paul
	datasets="pool/backups|backup/localbackupserver/backups"
'

# copy castor and pollux to dedicated pools with same names"
example1='
	ssh_host=10.32.0.11
	ssh_user=testuser
	datasets="castor|castor pollux|pollux"
'

# copy only var usr home without recursing into children datasets
# copy last snapshot that matches "fgrep _BACKUP_"
# override global "zfs send" with -v
example2='
	send_params=-v
	filter=fgrep
	filter_params="_BACKUP_"
	ssh_host=10.32.0.12
	ssh_user=testuser
	norecursion=1
	datasets="zeus/root/var|zeus/var zeus/root/usr|zeus/usr zeus/root/home|zeus/home"
'
